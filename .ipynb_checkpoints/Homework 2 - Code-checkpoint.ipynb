{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142cae77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sofia\n",
      "[nltk_data]     Beyerlein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sofia\n",
      "[nltk_data]     Beyerlein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sofia\n",
      "[nltk_data]     Beyerlein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Sofia\n",
      "[nltk_data]     Beyerlein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer as wnl\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "\n",
    "import torch.autograd as tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6df58fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download training and testing data\n",
    "file_path1 = r'C:\\Users\\Sofia Beyerlein\\Desktop\\Cornell Graduate\\Applied Machine Learning\\hw2\\test.csv'\n",
    "test = pd.read_csv(file_path1)\n",
    "test_ids = test['Id']\n",
    "\n",
    "file_path2 = r'C:\\Users\\Sofia Beyerlein\\Desktop\\Cornell Graduate\\Applied Machine Learning\\hw2\\train.csv'\n",
    "train = pd.read_csv(file_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b016dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the data, explain preprocessing steps and the reasons why you need to preprocess\n",
    "#cleaning training\n",
    "train['LotFrontage'].fillna(train['LotFrontage'].median(), inplace=True)\n",
    "train['LotArea'].fillna(train['LotArea'].median(), inplace=True)\n",
    "train['YearBuilt'].fillna(train['LotArea'].median(), inplace=True)\n",
    "train['LotShape'].fillna('None', inplace=True)\n",
    "train['BldgType'].fillna('None', inplace=True)\n",
    "train['HouseStyle'].fillna('None', inplace=True)\n",
    "\n",
    "#cleaning testing\n",
    "test['LotFrontage'].fillna(test['LotFrontage'].median(), inplace=True)\n",
    "test['LotArea'].fillna(test['LotArea'].median(), inplace=True)\n",
    "test['YearBuilt'].fillna(test['LotArea'].median(), inplace=True)\n",
    "test['LotShape'].fillna('None', inplace=True)\n",
    "test['BldgType'].fillna('None', inplace=True)\n",
    "test['HouseStyle'].fillna('None', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e671d92c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#one-hot encoding\n",
    "test_combined = pd.get_dummies(test, drop_first = True)\n",
    "train_combined = pd.get_dummies(train, drop_first= True)\n",
    "\n",
    "test_combined = test_combined.reindex(columns=train_combined.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1798b3",
   "metadata": {},
   "source": [
    "1. Continuation of House Prices from HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08f0b762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using OLS\n",
    "columns_to_include = ['LotFrontage', 'LotArea', 'LotShape_IR2', 'LotShape_IR3', 'LotShape_Reg', 'BldgType_2fmCon', 'BldgType_Duplex', 'BldgType_Twnhs', 'BldgType_TwnhsE', 'HouseStyle_1.5Unf', 'HouseStyle_1Story', 'HouseStyle_2.5Unf', 'HouseStyle_2Story', 'HouseStyle_SFoyer', 'HouseStyle_SLvl', 'YearBuilt']\n",
    "\n",
    "target = 'SalePrice'\n",
    "\n",
    "#turning the feats into tensors\n",
    "x_train = torch.tensor(train_combined[columns_to_include].shape[0], dtype=torch.float32)\n",
    "y_train = torch.tensor(train_combined[target].values, dtype=torch.float32)\n",
    "x_test = torch.tensor(test_combined[columns_to_include].shape[0], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e57dd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#train model on dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[1;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x, y)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#predict\u001b[39;00m\n\u001b[0;32m      6\u001b[0m y_prediction_complete \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "#train model on dataset\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "\n",
    "#predict\n",
    "y_prediction_complete = model.predict(test) #predicting on the actual test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1239ca18",
   "metadata": {},
   "source": [
    "2. Data generating distribution and convergence of linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fab6735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART A\n",
    "#made function so it could be used easily in next part\n",
    "\n",
    "def regression_r2(num_samples):\n",
    "    #setting up variables\n",
    "    mean_X = 168\n",
    "    std_X = 30\n",
    "    mean_eps = 0\n",
    "    std_eps = 20\n",
    "    alpha = 20\n",
    "    beta = 0.5\n",
    "\n",
    "    #filling X and epsilon\n",
    "    X = np.random.normal(mean_X, std_X, num_samples).reshape(-1, 1)\n",
    "    epsilon = np.random.normal(mean_eps, std_eps, num_samples).reshape(-1, 1)\n",
    "\n",
    "    #making Y function\n",
    "    Y = alpha + beta * X + epsilon\n",
    "    \n",
    "    #making prediction\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, Y)\n",
    "    y_predict = model.predict(X)\n",
    "\n",
    "    #calculating r2 \n",
    "    r2 = r2_score(Y, y_predict)\n",
    "    \n",
    "    return r2, model.coef_[0, 0], model.intercept_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3d807d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score, coefficient, intercept:\n",
      "(0.35787810705601775, 0.5039358201769829, 19.942450175454255)\n",
      "(0.3736254887395324, 0.5334528737340345, 14.810887895707694)\n",
      "(0.3751595869114398, 0.5103903307529633, 17.99413565917949)\n",
      "(0.3608703512238933, 0.5026798527587318, 19.4948190907038)\n",
      "(0.35925942472158756, 0.49947218308891306, 20.108372777508166)\n"
     ]
    }
   ],
   "source": [
    "#PART B\n",
    "\n",
    "print(\"R2 score, coefficient, intercept:\")\n",
    "print(regression_r2(pow(10, 2)))\n",
    "print(regression_r2(pow(10, 3)))\n",
    "print(regression_r2(pow(10, 4)))\n",
    "print(regression_r2(pow(10, 5)))\n",
    "print(regression_r2(pow(10, 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca476e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART C\n",
    "\n",
    "#Written in the Homework 2 - Report\n",
    "\n",
    "#PART D NOT DONE\n",
    "#Written in the Homework 2 - Report\n",
    "\n",
    "#PART E EXTRA CREDIT\n",
    "\n",
    "#PART F\n",
    "#Written in the Homework 2 - Report\n",
    "\n",
    "#PART G\n",
    "#Written in the Homework 2 - Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f1a8a4",
   "metadata": {},
   "source": [
    "3. Binary Classification on Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47bb1b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disaster percent:  42.96597924602653 no disaster percent:  57.03402075397347\n"
     ]
    }
   ],
   "source": [
    "#PART A\n",
    "\n",
    "#Questions answered in Homework 2 - Report\n",
    "filepath_train = (r\"C:\\Users\\Sofia Beyerlein\\Desktop\\Cornell Graduate\\Applied Machine Learning\\hw2\\nlp-getting-started\\train.csv\")\n",
    "filepath_test = (r'C:\\Users\\Sofia Beyerlein\\Desktop\\Cornell Graduate\\Applied Machine Learning\\hw2\\nlp-getting-started\\test.csv')\n",
    "\n",
    "train = pd.read_csv(filepath_train)\n",
    "test = pd.read_csv(filepath_test)\n",
    "\n",
    "disaster_count = train[train['target'] == 1].shape[0]\n",
    "no_disaster_count = train[train['target'] == 0].shape[0]\n",
    "disaster_percent = (disaster_count/len(train)) * 100\n",
    "no_disaster_percent = (no_disaster_count/len(train)) * 100\n",
    "print(\"disaster percent: \", disaster_percent, \"no disaster percent: \", no_disaster_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e21e199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART B\n",
    "\n",
    "#70% -> 5329/7613 and 30% -> 2284\n",
    "training_set = train.sample(frac=0.7)\n",
    "dev_set = train.drop(training_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de8f3aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent photo ruby smoke pours school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>heavy rain cause flash flooding street manitou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>im top hill see fire wood</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>haha south tampa getting flooded hah wait seco...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7585</th>\n",
       "      <td>10839</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>calgary police flood road closure calgary http...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7601</th>\n",
       "      <td>10859</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>refugio oil spill may costlier bigger projecte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7606</th>\n",
       "      <td>10866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>suicide bomber kill 15 saudi security site mos...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant crane holding bridge collapse nearby...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m194 0104 utc5km volcano hawaii httptcozdtoyd8ebj</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2284 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "2         5     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "6        10     NaN      NaN   \n",
       "7        13     NaN      NaN   \n",
       "11       17     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7585  10839     NaN      NaN   \n",
       "7601  10859     NaN      NaN   \n",
       "7606  10866     NaN      NaN   \n",
       "7608  10869     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "2     resident asked shelter place notified officer ...       1  \n",
       "4                got sent photo ruby smoke pours school       1  \n",
       "6     heavy rain cause flash flooding street manitou...       1  \n",
       "7                             im top hill see fire wood       1  \n",
       "11    haha south tampa getting flooded hah wait seco...       1  \n",
       "...                                                 ...     ...  \n",
       "7585  calgary police flood road closure calgary http...       1  \n",
       "7601  refugio oil spill may costlier bigger projecte...       1  \n",
       "7606  suicide bomber kill 15 saudi security site mos...       1  \n",
       "7608  two giant crane holding bridge collapse nearby...       1  \n",
       "7610  m194 0104 utc5km volcano hawaii httptcozdtoyd8ebj       1  \n",
       "\n",
       "[2284 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PART C\n",
    "\n",
    "def preprocess_data(df):\n",
    "    words_to_remove = {'the', 'and', 'or'}\n",
    "    #lowercase\n",
    "    df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "    #remove @ and urls\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'@\\S+', '', x))\n",
    "    #remove # and hashtags\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'#\\S+', '', x))\n",
    "    #strip punctuation\n",
    "    df['text'] = df['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "    #strip the and or\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(word for word in x.split() if word not in words_to_remove))\n",
    "    #lemmatize\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def lemmatize_text(text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "        return ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    df['text'] = df['text'].apply(lemmatize_text)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "        \n",
    "preprocess_data(training_set)\n",
    "preprocess_data(dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d2c5cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features: 1869\n"
     ]
    }
   ],
   "source": [
    "#PART D\n",
    "\n",
    "M = 3\n",
    "vectorizer = CountVectorizer(binary=True, min_df=M)\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(training_set['text'])\n",
    "dev_vectors = vectorizer.transform(dev_set['text'])\n",
    "\n",
    "num_features = len(vectorizer.get_feature_names_out())\n",
    "print(f\"Total number of features: {num_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7c3155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression without regularization:\n",
      "F1 score training: 0.9510930809105251\n",
      "F1 score dev: 0.6683143845773604\n",
      "\n",
      "Logistic Regression with L1 regularization:\n",
      "F1 score training: 0.8223668353820197\n",
      "F1 score dev: 0.7227346717308736\n",
      "\n",
      "Logistic Regression with L2 regularization:\n",
      "F1 score training: 0.8455089820359282\n",
      "F1 score dev: 0.7261078483715964\n",
      "\n",
      "Positive:\n",
      "typhoon : 4.315161192995197\n",
      "wildfire : 3.8060819664259316\n",
      "hiroshima : 3.3046233284259743\n",
      "spill : 2.959903699664655\n",
      "migrant : 2.9476537231460322\n",
      "\n",
      "Negative:\n",
      "republican : -2.3416731906628385\n",
      "character : -2.1406858163631273\n",
      "love : -1.9833625522399099\n",
      "ebay : -1.9441161683546115\n",
      "ruin : -1.9390339173765958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sofia Beyerlein\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#PART E\n",
    "\n",
    "#part i.\n",
    "lr_nr = LogisticRegression(penalty=None, max_iter=200)\n",
    "lr_nr.fit(train_vectors, training_set['target'])\n",
    "y_train_predict_nr = lr_nr.predict(train_vectors)\n",
    "y_dev_predict_nr = lr_nr.predict(dev_vectors)\n",
    "f1_train_nr = f1_score(training_set['target'], y_train_predict_nr)\n",
    "f1_dev_nr = f1_score(dev_set['target'], y_dev_predict_nr)\n",
    "print('Logistic Regression without regularization:')\n",
    "print(f\"F1 score training: {f1_train_nr}\")\n",
    "print(f\"F1 score dev: {f1_dev_nr}\")\n",
    "\n",
    "#part ii.\n",
    "lr_l1 = LogisticRegression(penalty='l1', solver = 'liblinear')\n",
    "lr_l1.fit(train_vectors, training_set['target'])\n",
    "y_train_predict_l1 = lr_l1.predict(train_vectors)\n",
    "y_dev_predict_l1 = lr_l1.predict(dev_vectors)\n",
    "f1_train_l1 = f1_score(training_set['target'], y_train_predict_l1)\n",
    "f1_dev_l1 = f1_score(dev_set['target'], y_dev_predict_l1)\n",
    "print('\\nLogistic Regression with L1 regularization:')\n",
    "print(f\"F1 score training: {f1_train_l1}\")\n",
    "print(f\"F1 score dev: {f1_dev_l1}\")\n",
    "\n",
    "#part iii.\n",
    "lr_l2 = LogisticRegression(penalty='l2', solver = 'liblinear')\n",
    "lr_l2.fit(train_vectors, training_set['target'])\n",
    "y_train_predict_l2 = lr_l2.predict(train_vectors)\n",
    "y_dev_predict_l2 = lr_l2.predict(dev_vectors)\n",
    "f1_train_l2 = f1_score(training_set['target'], y_train_predict_l2)\n",
    "f1_dev_l2 = f1_score(dev_set['target'], y_dev_predict_l2)\n",
    "print('\\nLogistic Regression with L2 regularization:')\n",
    "print(f\"F1 score training: {f1_train_l2}\")\n",
    "print(f\"F1 score dev: {f1_dev_l2}\")\n",
    "\n",
    "#part iv.\n",
    "#This part is written in the report\n",
    "\n",
    "#part v\n",
    "feat_names = vectorizer.get_feature_names_out()\n",
    "coefficients = lr_l1.coef_[0]\n",
    "\n",
    "positive = coefficients.argsort()[-5:][::-1]\n",
    "negative = coefficients.argsort()[:5]\n",
    "\n",
    "print(\"\\nPositive:\")\n",
    "for words in positive:\n",
    "    print(f\"{feat_names[words]} : {coefficients[words]}\")\n",
    "          \n",
    "print(\"\\nNegative:\")\n",
    "for words in negative:\n",
    "    print(f\"{feat_names[words]} : {coefficients[words]}\")         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "790d93b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of 2-grams in vocabulary: 1434\n",
      "\n",
      "Random 2-grams in vocab:\n",
      "dont miss\n",
      "pm cdt\n",
      "30 fire\n",
      "disaster declaration\n",
      "emergency response\n",
      "silver spring\n",
      "spot flood\n",
      "italian alp\n",
      "civilian casualty\n",
      "fog lamp\n",
      "\n",
      "Logistic regression with 2-gram:\n",
      "F1 score training: 0.6266706266706267\n",
      "F1 score dev: 0.5426039536468985\n"
     ]
    }
   ],
   "source": [
    "#PART F\n",
    "\n",
    "M = 3\n",
    "vec_2_gram = CountVectorizer(ngram_range=(2, 2), min_df=M)\n",
    "\n",
    "#fitting the data\n",
    "x_train_2_gram = vec_2_gram.fit_transform(training_set['text'])\n",
    "x_dev_2_gram = vec_2_gram.transform(dev_set['text'])\n",
    "\n",
    "#length of vocab\n",
    "vocab_len = len(vec_2_gram.get_feature_names_out())\n",
    "print(f\"Num of 2-grams in vocabulary: {vocab_len}\")\n",
    "\n",
    "#printing the pairs of n-gram\n",
    "rand_2_grams = np.random.choice(vec_2_gram.get_feature_names_out(), 10, replace=False)\n",
    "print(\"\\nRandom 2-grams in vocab:\")\n",
    "for thing in rand_2_grams:\n",
    "    print(thing)\n",
    "\n",
    "#training the models\n",
    "lr_2_gram = LogisticRegression(penalty='l2')\n",
    "lr_2_gram.fit(x_train_2_gram, training_set['target'])\n",
    "\n",
    "#predicting on training and dev set\n",
    "y_train_pred_2_gram = lr_2_gram.predict(x_train_2_gram)\n",
    "y_dev_pred_2_gram = lr_2_gram.predict(x_dev_2_gram)\n",
    "\n",
    "#f1 scores\n",
    "f1_train_2_gram = f1_score(training_set['target'], y_train_pred_2_gram)\n",
    "f1_dev_2_gram = f1_score(dev_set['target'], y_dev_pred_2_gram)\n",
    "\n",
    "print(\"\\nLogistic regression with 2-gram:\")\n",
    "print(f\"F1 score training: {f1_train_2_gram}\")\n",
    "print(f\"F1 score dev: {f1_dev_2_gram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09da53c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART G\n",
    "\n",
    "data_comb = pd.concat([training_set, dev_set], ignore_index=True)\n",
    "\n",
    "M = 3\n",
    "vectorizer = CountVectorizer(binary=True, min_df=M)\n",
    "x_complete = vectorizer.fit_transform(data_comb['text'])\n",
    "\n",
    "# Train the logistic regression model with L1 regularization\n",
    "lr_l1 = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "lr_l1.fit(x_complete, data_comb['target'])\n",
    "\n",
    "# Predict on the train and test sets\n",
    "x_test = vectorizer.transform(test['text'])\n",
    "y_pred = lr_l1.predict(x_test)\n",
    "\n",
    "final = pd.DataFrame({'id': test['id'], 'target': y_pred})\n",
    "final.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
