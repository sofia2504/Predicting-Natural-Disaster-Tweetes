{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142cae77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sofia\n",
      "[nltk_data]     Beyerlein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sofia\n",
      "[nltk_data]     Beyerlein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sofia\n",
      "[nltk_data]     Beyerlein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Sofia\n",
      "[nltk_data]     Beyerlein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer as wnl\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "\n",
    "import torch.autograd as tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6df58fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download training and testing data\n",
    "file_path1 = r'C:\\Users\\Sofia Beyerlein\\Desktop\\Cornell Graduate\\Applied Machine Learning\\hw2\\test.csv'\n",
    "test = pd.read_csv(file_path1)\n",
    "test_ids = test['Id']\n",
    "\n",
    "file_path2 = r'C:\\Users\\Sofia Beyerlein\\Desktop\\Cornell Graduate\\Applied Machine Learning\\hw2\\train.csv'\n",
    "train = pd.read_csv(file_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b016dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the data, explain preprocessing steps and the reasons why you need to preprocess\n",
    "#cleaning training\n",
    "train['LotFrontage'].fillna(train['LotFrontage'].median(), inplace=True)\n",
    "train['LotArea'].fillna(train['LotArea'].median(), inplace=True)\n",
    "train['YearBuilt'].fillna(train['LotArea'].median(), inplace=True)\n",
    "train['LotShape'].fillna('None', inplace=True)\n",
    "train['BldgType'].fillna('None', inplace=True)\n",
    "train['HouseStyle'].fillna('None', inplace=True)\n",
    "\n",
    "#cleaning testing\n",
    "test['LotFrontage'].fillna(test['LotFrontage'].median(), inplace=True)\n",
    "test['LotArea'].fillna(test['LotArea'].median(), inplace=True)\n",
    "test['YearBuilt'].fillna(test['LotArea'].median(), inplace=True)\n",
    "test['LotShape'].fillna('None', inplace=True)\n",
    "test['BldgType'].fillna('None', inplace=True)\n",
    "test['HouseStyle'].fillna('None', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e671d92c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#one-hot encoding\n",
    "test_combined = pd.get_dummies(test, drop_first = True)\n",
    "train_combined = pd.get_dummies(train, drop_first= True)\n",
    "\n",
    "test_combined = test_combined.reindex(columns=train_combined.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1798b3",
   "metadata": {},
   "source": [
    "1. Continuation of House Prices from HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08f0b762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in x_train: False\n",
      "NaN values in y_train: False\n",
      "MSE: 2400779520.0\n",
      "Root MSE: 48997.75\n",
      "r^2 scoreL 0.6193340572519818\n"
     ]
    }
   ],
   "source": [
    "#PART A\n",
    "\n",
    "# Using OLS\n",
    "columns_to_include = ['LotFrontage', 'LotArea', 'LotShape_IR2', 'LotShape_IR3', 'LotShape_Reg', 'BldgType_2fmCon', 'BldgType_Duplex', 'BldgType_Twnhs', 'BldgType_TwnhsE', 'HouseStyle_1.5Unf', 'HouseStyle_1Story', 'HouseStyle_2.5Unf', 'HouseStyle_2Story', 'HouseStyle_SFoyer', 'HouseStyle_SLvl', 'YearBuilt']\n",
    "\n",
    "target = 'SalePrice'\n",
    "\n",
    "train_combined = train_combined.drop(columns=['GarageYrBlt'])\n",
    "train_combined = train_combined.drop(columns=['MasVnrArea'])\n",
    "\n",
    "#turning the feats into tensors\n",
    "x_train = np.array(train_combined.drop(columns = [target]), dtype=np.float32)\n",
    "y_train = np.array(train_combined[target], dtype=np.float32)\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1,1)\n",
    "\n",
    "print(\"NaN values in x_train:\", torch.isnan(x_train).any().item())\n",
    "print(\"NaN values in y_train:\", torch.isnan(y_train).any().item())\n",
    "\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "#model instance\n",
    "input_dim = x_train.shape[1]\n",
    "model = LinearRegression(input_dim)\n",
    "\n",
    "#loss\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "#train model\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(x_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_train).numpy()\n",
    "\n",
    "#mse\n",
    "mse = mean_squared_error(y_train.numpy(), y_pred)\n",
    "root_mse = np.sqrt(mse)\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"Root MSE: {root_mse}\")\n",
    "\n",
    "#r^2\n",
    "r2 = r2_score(y_train.numpy(), y_pred)\n",
    "print(f\"r^2 scoreL {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "070f496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART B\n",
    "#In the HW2 - Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1239ca18",
   "metadata": {},
   "source": [
    "2. Data generating distribution and convergence of linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fab6735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART A\n",
    "#made function so it could be used easily in next part\n",
    "\n",
    "def regression_r2(num_samples):\n",
    "    #setting up variables\n",
    "    mean_X = 168\n",
    "    std_X = 30\n",
    "    mean_eps = 0\n",
    "    std_eps = 20\n",
    "    alpha = 20\n",
    "    beta = 0.5\n",
    "\n",
    "    #filling X and epsilon\n",
    "    X = np.random.normal(mean_X, std_X, num_samples).reshape(-1, 1)\n",
    "    epsilon = np.random.normal(mean_eps, std_eps, num_samples).reshape(-1, 1)\n",
    "\n",
    "    #making Y function\n",
    "    Y = alpha + beta * X + epsilon\n",
    "    \n",
    "    #making prediction\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, Y)\n",
    "    y_predict = model.predict(X)\n",
    "\n",
    "    #calculating r2 \n",
    "    r2 = r2_score(Y, y_predict)\n",
    "    \n",
    "    return r2, model.coef_[0, 0], model.intercept_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b3d807d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score, coefficient, intercept:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LinearRegression.__init__() missing 1 required positional argument: 'input_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#PART B\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR2 score, coefficient, intercept:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(regression_r2(\u001b[38;5;28mpow\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m2\u001b[39m)))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(regression_r2(\u001b[38;5;28mpow\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m3\u001b[39m)))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(regression_r2(\u001b[38;5;28mpow\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m4\u001b[39m)))\n",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m, in \u001b[0;36mregression_r2\u001b[1;34m(num_samples)\u001b[0m\n\u001b[0;32m     18\u001b[0m Y \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m+\u001b[39m beta \u001b[38;5;241m*\u001b[39m X \u001b[38;5;241m+\u001b[39m epsilon\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#making prediction\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[0;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X, Y)\n\u001b[0;32m     23\u001b[0m y_predict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X)\n",
      "\u001b[1;31mTypeError\u001b[0m: LinearRegression.__init__() missing 1 required positional argument: 'input_dim'"
     ]
    }
   ],
   "source": [
    "#PART B\n",
    "\n",
    "print(\"R2 score, coefficient, intercept:\")\n",
    "print(regression_r2(pow(10, 2)))\n",
    "print(regression_r2(pow(10, 3)))\n",
    "print(regression_r2(pow(10, 4)))\n",
    "print(regression_r2(pow(10, 5)))\n",
    "print(regression_r2(pow(10, 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca476e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART C\n",
    "\n",
    "#Written in the Homework 2 - Report\n",
    "\n",
    "#PART D\n",
    "#Written in the Homework 2 - Report\n",
    "\n",
    "#PART E EXTRA CREDIT\n",
    "\n",
    "#PART F\n",
    "#Written in the Homework 2 - Report\n",
    "\n",
    "#PART G\n",
    "#Written in the Homework 2 - Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f1a8a4",
   "metadata": {},
   "source": [
    "3. Binary Classification on Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47bb1b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disaster percent:  42.96597924602653 no disaster percent:  57.03402075397347\n"
     ]
    }
   ],
   "source": [
    "#PART A\n",
    "\n",
    "#Questions answered in Homework 2 - Report\n",
    "filepath_train = (r\"C:\\Users\\Sofia Beyerlein\\Desktop\\Cornell Graduate\\Applied Machine Learning\\hw2\\nlp-getting-started\\train.csv\")\n",
    "filepath_test = (r'C:\\Users\\Sofia Beyerlein\\Desktop\\Cornell Graduate\\Applied Machine Learning\\hw2\\nlp-getting-started\\test.csv')\n",
    "\n",
    "train = pd.read_csv(filepath_train)\n",
    "test = pd.read_csv(filepath_test)\n",
    "\n",
    "disaster_count = train[train['target'] == 1].shape[0]\n",
    "no_disaster_count = train[train['target'] == 0].shape[0]\n",
    "disaster_percent = (disaster_count/len(train)) * 100\n",
    "no_disaster_percent = (no_disaster_count/len(train)) * 100\n",
    "print(\"disaster percent: \", disaster_percent, \"no disaster percent: \", no_disaster_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e21e199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART B\n",
    "\n",
    "#70% -> 5329/7613 and 30% -> 2284\n",
    "training_set = train.sample(frac=0.7)\n",
    "dev_set = train.drop(training_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de8f3aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason may allah forgive u</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent photo ruby smoke pours school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>im top hill see fire wood</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7591</th>\n",
       "      <td>10846</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>heat wave warning aa ayyo dei plan visit frien...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>10853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fatherofthree lost control car overtaking coll...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7600</th>\n",
       "      <td>10855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>evacuation order lifted town roosevelt httptco...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7603</th>\n",
       "      <td>10862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>official say quarantine place alabama home pos...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest home razed northern california wildfire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2284 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "7        13     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7591  10846     NaN      NaN   \n",
       "7598  10853     NaN      NaN   \n",
       "7600  10855     NaN      NaN   \n",
       "7603  10862     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0                       deed reason may allah forgive u       1  \n",
       "1                 forest fire near la ronge sask canada       1  \n",
       "2     resident asked shelter place notified officer ...       1  \n",
       "4                got sent photo ruby smoke pours school       1  \n",
       "7                             im top hill see fire wood       1  \n",
       "...                                                 ...     ...  \n",
       "7591  heat wave warning aa ayyo dei plan visit frien...       1  \n",
       "7598  fatherofthree lost control car overtaking coll...       1  \n",
       "7600  evacuation order lifted town roosevelt httptco...       1  \n",
       "7603  official say quarantine place alabama home pos...       1  \n",
       "7612  latest home razed northern california wildfire...       1  \n",
       "\n",
       "[2284 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PART C\n",
    "\n",
    "def preprocess_data(df):\n",
    "    words_to_remove = {'the', 'and', 'or'}\n",
    "    #lowercase\n",
    "    df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "    #remove @ and urls\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'@\\S+', '', x))\n",
    "    #remove # and hashtags\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'#\\S+', '', x))\n",
    "    #strip punctuation\n",
    "    df['text'] = df['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "    #strip the and or\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(word for word in x.split() if word not in words_to_remove))\n",
    "    #lemmatize\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def lemmatize_text(text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "        return ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    df['text'] = df['text'].apply(lemmatize_text)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "        \n",
    "preprocess_data(training_set)\n",
    "preprocess_data(dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d2c5cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features: 3020\n"
     ]
    }
   ],
   "source": [
    "#PART D\n",
    "\n",
    "M = 3\n",
    "vectorizer = CountVectorizer(binary=True, min_df=M)\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(training_set['text'])\n",
    "dev_vectors = vectorizer.transform(dev_set['text'])\n",
    "\n",
    "num_features = len(vectorizer.get_feature_names_out())\n",
    "print(f\"Total number of features: {num_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7c3155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression without regularization:\n",
      "F1 score training: 0.9774271312732852\n",
      "F1 score dev: 0.6596279537456008\n",
      "\n",
      "Logistic Regression with L1 regularization:\n",
      "F1 score training: 0.8401210146613916\n",
      "F1 score dev: 0.706992230854606\n",
      "\n",
      "Logistic Regression with L2 regularization:\n",
      "F1 score training: 0.8869604932633022\n",
      "F1 score dev: 0.7237991266375545\n",
      "\n",
      "Positive:\n",
      "typhoon : 3.815250112567738\n",
      "migrant : 3.5704044448005754\n",
      "bombing : 3.4775784559068135\n",
      "drought : 2.9886744754565275\n",
      "mh370 : 2.9767008480840738\n",
      "\n",
      "Negative:\n",
      "dying : -2.191539628516657\n",
      "bag : -1.971195526669905\n",
      "armageddon : -1.9593554715689157\n",
      "love : -1.9474393314500993\n",
      "anti : -1.8716101580586273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sofia Beyerlein\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#PART E\n",
    "\n",
    "#part i.\n",
    "lr_nr = LogisticRegression(penalty=None, max_iter=200)\n",
    "lr_nr.fit(train_vectors, training_set['target'])\n",
    "y_train_predict_nr = lr_nr.predict(train_vectors)\n",
    "y_dev_predict_nr = lr_nr.predict(dev_vectors)\n",
    "f1_train_nr = f1_score(training_set['target'], y_train_predict_nr)\n",
    "f1_dev_nr = f1_score(dev_set['target'], y_dev_predict_nr)\n",
    "print('Logistic Regression without regularization:')\n",
    "print(f\"F1 score training: {f1_train_nr}\")\n",
    "print(f\"F1 score dev: {f1_dev_nr}\")\n",
    "\n",
    "#part ii.\n",
    "lr_l1 = LogisticRegression(penalty='l1', solver = 'liblinear')\n",
    "lr_l1.fit(train_vectors, training_set['target'])\n",
    "y_train_predict_l1 = lr_l1.predict(train_vectors)\n",
    "y_dev_predict_l1 = lr_l1.predict(dev_vectors)\n",
    "f1_train_l1 = f1_score(training_set['target'], y_train_predict_l1)\n",
    "f1_dev_l1 = f1_score(dev_set['target'], y_dev_predict_l1)\n",
    "print('\\nLogistic Regression with L1 regularization:')\n",
    "print(f\"F1 score training: {f1_train_l1}\")\n",
    "print(f\"F1 score dev: {f1_dev_l1}\")\n",
    "\n",
    "#part iii.\n",
    "lr_l2 = LogisticRegression(penalty='l2', solver = 'liblinear')\n",
    "lr_l2.fit(train_vectors, training_set['target'])\n",
    "y_train_predict_l2 = lr_l2.predict(train_vectors)\n",
    "y_dev_predict_l2 = lr_l2.predict(dev_vectors)\n",
    "f1_train_l2 = f1_score(training_set['target'], y_train_predict_l2)\n",
    "f1_dev_l2 = f1_score(dev_set['target'], y_dev_predict_l2)\n",
    "print('\\nLogistic Regression with L2 regularization:')\n",
    "print(f\"F1 score training: {f1_train_l2}\")\n",
    "print(f\"F1 score dev: {f1_dev_l2}\")\n",
    "\n",
    "#part iv.\n",
    "#This part is written in the report\n",
    "\n",
    "#part v\n",
    "feat_names = vectorizer.get_feature_names_out()\n",
    "coefficients = lr_l1.coef_[0]\n",
    "\n",
    "positive = coefficients.argsort()[-5:][::-1]\n",
    "negative = coefficients.argsort()[:5]\n",
    "\n",
    "print(\"\\nPositive:\")\n",
    "for words in positive:\n",
    "    print(f\"{feat_names[words]} : {coefficients[words]}\")\n",
    "          \n",
    "print(\"\\nNegative:\")\n",
    "for words in negative:\n",
    "    print(f\"{feat_names[words]} : {coefficients[words]}\")         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "790d93b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of 2-grams in vocabulary: 1504\n",
      "\n",
      "Random 2-grams in vocab:\n",
      "added video\n",
      "terror attack\n",
      "modified land\n",
      "freakiest freak\n",
      "saved many\n",
      "prime minister\n",
      "politics grief\n",
      "20 amp\n",
      "sandstorm minute\n",
      "colorado theater\n",
      "\n",
      "Logistic regression with 2-gram:\n",
      "F1 score training: 0.6334661354581673\n",
      "F1 score dev: 0.5505780346820809\n"
     ]
    }
   ],
   "source": [
    "#PART F\n",
    "\n",
    "M = 3\n",
    "vec_2_gram = CountVectorizer(ngram_range=(2, 2), min_df=M)\n",
    "\n",
    "#fitting the data\n",
    "x_train_2_gram = vec_2_gram.fit_transform(training_set['text'])\n",
    "x_dev_2_gram = vec_2_gram.transform(dev_set['text'])\n",
    "\n",
    "#length of vocab\n",
    "vocab_len = len(vec_2_gram.get_feature_names_out())\n",
    "print(f\"Num of 2-grams in vocabulary: {vocab_len}\")\n",
    "\n",
    "#printing the pairs of n-gram\n",
    "rand_2_grams = np.random.choice(vec_2_gram.get_feature_names_out(), 10, replace=False)\n",
    "print(\"\\nRandom 2-grams in vocab:\")\n",
    "for thing in rand_2_grams:\n",
    "    print(thing)\n",
    "\n",
    "#training the models\n",
    "lr_2_gram = LogisticRegression(penalty='l2')\n",
    "lr_2_gram.fit(x_train_2_gram, training_set['target'])\n",
    "\n",
    "#predicting on training and dev set\n",
    "y_train_pred_2_gram = lr_2_gram.predict(x_train_2_gram)\n",
    "y_dev_pred_2_gram = lr_2_gram.predict(x_dev_2_gram)\n",
    "\n",
    "#f1 scores\n",
    "f1_train_2_gram = f1_score(training_set['target'], y_train_pred_2_gram)\n",
    "f1_dev_2_gram = f1_score(dev_set['target'], y_dev_pred_2_gram)\n",
    "\n",
    "print(\"\\nLogistic regression with 2-gram:\")\n",
    "print(f\"F1 score training: {f1_train_2_gram}\")\n",
    "print(f\"F1 score dev: {f1_dev_2_gram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09da53c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART G\n",
    "\n",
    "data_comb = pd.concat([training_set, dev_set], ignore_index=True)\n",
    "\n",
    "M = 3\n",
    "vectorizer = CountVectorizer(binary=True, min_df=M)\n",
    "x_complete = vectorizer.fit_transform(data_comb['text'])\n",
    "\n",
    "# Train the logistic regression model with L1 regularization\n",
    "lr_l1 = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "lr_l1.fit(x_complete, data_comb['target'])\n",
    "\n",
    "# Predict on the train and test sets\n",
    "x_test = vectorizer.transform(test['text'])\n",
    "y_pred = lr_l1.predict(x_test)\n",
    "\n",
    "final = pd.DataFrame({'id': test['id'], 'target': y_pred})\n",
    "final.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
